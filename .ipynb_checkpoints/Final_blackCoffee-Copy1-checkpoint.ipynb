{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ade2ad-4fcc-4fc9-8545-4d1bb2316b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import string\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf9bc13d-b21f-411c-8f51-67ca0afd3783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download()\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('cmudict')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4c479bc-3583-4581-9008-dff0cb4006d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service()\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = webdriver.Chrome(service=service, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e2235e-89ae-4ec0-bc87-6c6b74c1234a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bctech2011</td>\n",
       "      <td>https://insights.blackcoffer.com/ml-and-ai-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bctech2012</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bctech2013</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bctech2014</td>\n",
       "      <td>https://insights.blackcoffer.com/effective-man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bctech2015</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       URL_ID                                                URL\n",
       "0  bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...\n",
       "1  bctech2012  https://insights.blackcoffer.com/streamlined-i...\n",
       "2  bctech2013  https://insights.blackcoffer.com/efficient-dat...\n",
       "3  bctech2014  https://insights.blackcoffer.com/effective-man...\n",
       "4  bctech2015  https://insights.blackcoffer.com/streamlined-t..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_excel('C:\\\\Users\\\\dell\\\\Downloads\\\\Input.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b043ad9-abbc-41d1-9edd-f5b621046b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['URL'])):\n",
    "    driver.get(data['URL'][i])\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3999b9b5-7c63-4a3d-b52b-3043d6054aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://insights.blackcoffer.com/automated-orthopedic-case-report-generation-harnessing-web-scraping-and-ai-integration/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4099f3e-bea7-46d4-beea-5fc4b6a0c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texter = driver.find_element('xpath', './/div[@class=\"td-pb-span8 td-main-content\"]//div[@class=\"td-ss-main-content\"]//div[@class=\"td-post-content tagdiv-type\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0597293-ae23-4274-a19b-af7f24eabc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=texter.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cec61b69-c94f-49d3-8225-5f32800ff303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client background client a leading healthtech firm in the usa industry type healthcare products services medical solutions, healthcare organization size the problem the problem is to efficiently create orthopedic case reports by extracting data from online sources, including articles, videos, and user comments. it involves summarizing and citing relevant articles from pubmed.gov for the past years related to the case. this requires automating the extraction and summarization of data from websites, making it a timeconsuming task if done manually. our solution develops a python tool that accepts a website url as input and generates a case report. integrates web scraping to extract data from websites. utilizes ai, such as chatgpt, for creating summaries and responses. leverages pubmed for citing and summarizing recent articles. provides a web application for userfriendly access to these capabilities. solution architecture utilizes web scraping techniques to gather data from trusted medical websites. combines web scraping with ai, including chatgpt, for generating case reports and responding to queries. utilizes pubmed for retrieving and summarizing recent articles related to the case. deploys a web application for user interaction and input. deliverables project github source code tech stack tools used chatgpt beautifulsoup requests languagetechniques used python models used none skills used python webscraping chatgpt prompting databases used none web cloud servers used none what are the technical challenges faced during project execution accurate and reliable web scraping from diverse medical websites. integration of ai components for text generation and summarization. efficient querying and retrieval of articles from pubmed. handling different data formats and structures from various online sources. developing a userfriendly web interface for input and interaction. how the technical challenges were solved extensive research and testing of web scraping techniques for medical websites. integration of ai models and libraries for text generation. utilization of pubmed api for article retrieval and summarization. custom data parsers for handling diverse data structures. collaboration with medical experts for user interface design and feedback. summarize summarized this project was done by the blackcoffer team, a global it consulting firm. contact details this solution was designed and developed by blackcoffer team here are my contact details firm name blackcoffer pvt. ltd. firm website www.blackcoffer.com firm address , eextension, shaym vihar phase , new delhi email ajayblackcoffer.com skype asbidyarthy whatsapp telegram asbidyarthy\n"
     ]
    }
   ],
   "source": [
    "text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "text = re.sub(r'\\[\\d+\\]', '', text)  # Remove references like [0-9]\n",
    "text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "text = re.sub(r'\\.\\s+', '. ', text)  # Fix extra spaces around periods\n",
    "text = re.sub(r'[^\\w\\s,.!?]', '', text)  # Remove unwanted punctuation\n",
    "text = re.sub(r'\\s+', ' ', text).strip()  # Remove redundant spaces\n",
    "text = re.sub(r'\\.\\s+\\.', '.', text)\n",
    "text = text.lower()  # Convert to lowercase\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25bd5f27-b7e0-4c27-94c0-b8b010a4678d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined stopwords have been saved to stopwords.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the folder path containing stopword files\n",
    "folder_path = r'C:\\Users\\dell\\Downloads\\StopWords-20240906T064651Z-001\\StopWords'  \n",
    "\n",
    "# Set to store unique stopwords\n",
    "all_stopwords = set()\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    # Check if it's a file (and not a directory)\n",
    "    if os.path.isfile(file_path):\n",
    "        try:\n",
    "            # Open and read the file, adding words to the set\n",
    "            with open(file_path, 'r') as file:\n",
    "                stopwords = file.read().splitlines()\n",
    "                all_stopwords.update(stopwords)  # Add to the set to keep only unique words\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "\n",
    "# Write all collected stopwords into a single 'stopwords.txt' file\n",
    "output_file = 'stopwords.txt'  # Name of the output file\n",
    "with open(output_file, 'w') as output:\n",
    "    for word in sorted(all_stopwords): \n",
    "        # word =word.lower()               # Sorting for better readability (optional)\n",
    "        output.write(word + '\\n')\n",
    "\n",
    "print(f\"Combined stopwords have been saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f0a1ac3-f93c-459e-ba4b-f7cf1d81808e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['client background client a leading healthtech firm in the usa industry type healthcare products services medical solutions, healthcare organization size the problem the problem is to efficiently create orthopedic case reports by extracting data from online sources, including articles, videos, and user comments.',\n",
       " 'it involves summarizing and citing relevant articles from pubmed.gov for the past years related to the case.',\n",
       " 'this requires automating the extraction and summarization of data from websites, making it a timeconsuming task if done manually.',\n",
       " 'our solution develops a python tool that accepts a website url as input and generates a case report.',\n",
       " 'integrates web scraping to extract data from websites.',\n",
       " 'utilizes ai, such as chatgpt, for creating summaries and responses.',\n",
       " 'leverages pubmed for citing and summarizing recent articles.',\n",
       " 'provides a web application for userfriendly access to these capabilities.',\n",
       " 'solution architecture utilizes web scraping techniques to gather data from trusted medical websites.',\n",
       " 'combines web scraping with ai, including chatgpt, for generating case reports and responding to queries.',\n",
       " 'utilizes pubmed for retrieving and summarizing recent articles related to the case.',\n",
       " 'deploys a web application for user interaction and input.',\n",
       " 'deliverables project github source code tech stack tools used chatgpt beautifulsoup requests languagetechniques used python models used none skills used python webscraping chatgpt prompting databases used none web cloud servers used none what are the technical challenges faced during project execution accurate and reliable web scraping from diverse medical websites.',\n",
       " 'integration of ai components for text generation and summarization.',\n",
       " 'efficient querying and retrieval of articles from pubmed.',\n",
       " 'handling different data formats and structures from various online sources.',\n",
       " 'developing a userfriendly web interface for input and interaction.',\n",
       " 'how the technical challenges were solved extensive research and testing of web scraping techniques for medical websites.',\n",
       " 'integration of ai models and libraries for text generation.',\n",
       " 'utilization of pubmed api for article retrieval and summarization.',\n",
       " 'custom data parsers for handling diverse data structures.',\n",
       " 'collaboration with medical experts for user interface design and feedback.',\n",
       " 'summarize summarized this project was done by the blackcoffer team, a global it consulting firm.',\n",
       " 'contact details this solution was designed and developed by blackcoffer team here are my contact details firm name blackcoffer pvt.',\n",
       " 'ltd. firm website www.blackcoffer.com firm address , eextension, shaym vihar phase , new delhi email ajayblackcoffer.com skype asbidyarthy whatsapp telegram asbidyarthy']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60aff601-4de0-4a9c-a632-28669dab6e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client background client leading healthtech firm usa industry type healthcare products services medical solutions , healthcare organization size problem problem efficiently create orthopedic case reports extracting data online sources , including articles , videos , user comments . involves summarizing citing relevant articles pubmed.gov past years related case . requires automating extraction summarization data websites , making timeconsuming task manually . solution develops python tool accepts website url input generates case report . integrates web scraping extract data websites . utilizes ai , chatgpt , creating summaries responses . leverages pubmed citing summarizing recent articles . web application userfriendly access capabilities . solution architecture utilizes web scraping techniques gather data trusted medical websites . combines web scraping ai , including chatgpt , generating case reports responding queries . utilizes pubmed retrieving summarizing recent articles related case . deploys web application user interaction input . deliverables project github source code tech stack tools chatgpt beautifulsoup requests languagetechniques python models skills python webscraping chatgpt prompting databases web cloud servers technical challenges faced project execution accurate reliable web scraping diverse medical websites . integration ai components text generation summarization . efficient querying retrieval articles pubmed . handling data formats structures online sources . developing userfriendly web interface input interaction . technical challenges solved extensive research testing web scraping techniques medical websites . integration ai models libraries text generation . utilization pubmed api article retrieval summarization . custom data parsers handling diverse data structures . collaboration medical experts user interface design feedback . summarize summarized project blackcoffer team , global consulting firm . contact details solution designed developed blackcoffer team contact details firm blackcoffer pvt . ltd. firm website www.blackcoffer.com firm address , eextension , shaym vihar phase , delhi email ajayblackcoffer.com skype asbidyarthy whatsapp telegram asbidyarthy\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "with open('stopwords.txt', 'r') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "# Lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    sentences[i] = ' '.join(words) \n",
    "# print(sentences)\n",
    "text = ' '.join(sentences)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d56f2a54-285a-4924-8223-399b778954a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Score: 3\n",
      "Negative Score: 6\n",
      "Polarity Score: -0.33333329629630043\n",
      "Subjectivity Score: 0.030716723444652823\n"
     ]
    }
   ],
   "source": [
    "def read_words(file_path):\n",
    "    \"\"\"Read words from a file and return as a set.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        words = set(word.strip().lower() for word in file.readlines())\n",
    "    return words\n",
    "\n",
    "def compute_sentiment_scores(text, positive_words, negative_words):\n",
    "    \"\"\"Compute positive and negative scores based on word lists.\"\"\"\n",
    "    # Tokenize the text\n",
    "    tokens = text.lower().split()\n",
    "    \n",
    "    # Count positive and negative words\n",
    "    positive_score = sum(1 for word in tokens if word in positive_words)\n",
    "    negative_score = sum(-1 for word in tokens if word in negative_words)\n",
    "    # Convert negative score to a positive value\n",
    "    negative_score = abs(negative_score)\n",
    "    \n",
    "    return positive_score, negative_score, tokens\n",
    "\n",
    "def calculate_polarity_score(positive_score, negative_score):\n",
    "    \"\"\"Calculate Polarity Score.\"\"\"\n",
    "    return (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "\n",
    "def calculate_subjectivity_score(positive_score, negative_score, total_words):\n",
    "    \"\"\"Calculate Subjectivity Score.\"\"\"\n",
    "    return (positive_score + negative_score) / (total_words + 0.000001)\n",
    "    \n",
    "# File paths\n",
    "positive_file = r'C:\\Users\\dell\\Downloads\\MasterDictionary-20240906T064652Z-001\\MasterDictionary\\negative-words.txt'\n",
    "negative_file = r'C:\\Users\\dell\\Downloads\\MasterDictionary-20240906T064652Z-001\\MasterDictionary\\positive-words.txt'\n",
    "\n",
    "# Read positive and negative words from files\n",
    "positive_words = read_words(positive_file)\n",
    "negative_words = read_words(negative_file)\n",
    "\n",
    "# Compute sentiment scores\n",
    "positive_score, negative_score, tokens = compute_sentiment_scores(text, positive_words, negative_words)\n",
    "\n",
    "# Calculate Polarity Score\n",
    "polarity_score = calculate_polarity_score(positive_score, negative_score)\n",
    "\n",
    "# Calculate Subjectivity Score\n",
    "subjectivity_score = calculate_subjectivity_score(positive_score, negative_score, len(tokens))\n",
    "\n",
    "# Display the results\n",
    "print(\"Positive Score:\", positive_score)\n",
    "print(\"Negative Score:\", negative_score)\n",
    "print(\"Polarity Score:\", polarity_score)\n",
    "print(\"Subjectivity Score:\", subjectivity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3930912-0d9e-4047-b9d5-7baf3d99da4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sentence Length: 11.72\n",
      "Percentage of Complex Words: 27.65%\n",
      "Fog Index: 15.75\n",
      "Complex Word Count: 81\n"
     ]
    }
   ],
   "source": [
    "# Load CMU Pronouncing Dictionary\n",
    "d = cmudict.dict()\n",
    "\n",
    "def count_syllables(word):\n",
    "    try:\n",
    "        syllable_counts = [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]]\n",
    "        return syllable_counts[0] if syllable_counts else 0\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def calculate_average_sentence_length(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    num_words = len(words)\n",
    "    \n",
    "    average_sentence_length = num_words / num_sentences if num_sentences > 0 else 0\n",
    "    average_sentence_length=round(average_sentence_length, 2)\n",
    "    return average_sentence_length\n",
    "\n",
    "def calculate_percentage_of_complex_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    num_words = len(words)\n",
    "    \n",
    "    complex_word_count = sum(1 for word in words if count_syllables(word) >= 3)\n",
    "    percentage_of_complex_words = (complex_word_count / num_words) * 100 if num_words > 0 else 0\n",
    "    percentage_of_complex_words=round(percentage_of_complex_words, 2)\n",
    "    return percentage_of_complex_words\n",
    "\n",
    "def calculate_fog_index(text):\n",
    "    average_sentence_length = calculate_average_sentence_length(text)\n",
    "    percentage_of_complex_words = calculate_percentage_of_complex_words(text)\n",
    "    \n",
    "    fog_index = 0.4 * (average_sentence_length + percentage_of_complex_words)\n",
    "    fog_index=round(fog_index, 2)\n",
    "    return fog_index\n",
    "\n",
    "# Calculate metrics\n",
    "average_sentence_length = calculate_average_sentence_length(text)\n",
    "percentage_of_complex_words = calculate_percentage_of_complex_words(text)\n",
    "fog_index = calculate_fog_index(text)\n",
    "# Calculate Complex Word Count\n",
    "complex_word_count = sum(1 for word in word_tokenize(text) if count_syllables(word) > 2)\n",
    "\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Average Sentence Length: {average_sentence_length}\")\n",
    "print(f\"Percentage of Complex Words: {percentage_of_complex_words}%\")\n",
    "print(f\"Fog Index: {fog_index}\")\n",
    "print(f\"Complex Word Count: {complex_word_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f53f78b8-58fc-4e6f-b061-64ab9b447727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client background client leading healthtech firm usa industry type healthcare products services medical solutions healthcare organization size problem problem efficiently create orthopedic case reports extracting data online sources including articles videos user comments . involves summarizing citing relevant articles pubmed.gov past years related case . requires automating extraction summarization data websites making timeconsuming task manually . solution develops python tool accepts website url input generates case report . integrates web scraping extract data websites . utilizes ai chatgpt creating summaries responses . leverages pubmed citing summarizing recent articles . web application userfriendly access capabilities . solution architecture utilizes web scraping techniques gather data trusted medical websites . combines web scraping ai including chatgpt generating case reports responding queries . utilizes pubmed retrieving summarizing recent articles related case . deploys web application user interaction input . deliverables project github source code tech stack tools chatgpt beautifulsoup requests languagetechniques python models skills python webscraping chatgpt prompting databases web cloud servers technical challenges faced project execution accurate reliable web scraping diverse medical websites . integration ai components text generation summarization . efficient querying retrieval articles pubmed . handling data formats structures online sources . developing userfriendly web interface input interaction . technical challenges solved extensive research testing web scraping techniques medical websites . integration ai models libraries text generation . utilization pubmed api article retrieval summarization . custom data parsers handling diverse data structures . collaboration medical experts user interface design feedback . summarize summarized project blackcoffer team global consulting firm . contact details solution designed developed blackcoffer team contact details firm blackcoffer pvt . ltd. firm website www.blackcoffer.com firm address eextension shaym vihar phase delhi email ajayblackcoffer.com skype asbidyarthy whatsapp telegram asbidyarthy\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Preprocessing the data\n",
    "texter_content = texter.text \n",
    "\n",
    "# Remove references like [0-9]\n",
    "text = re.sub(r'\\[[0-9]*\\]', '', text)\n",
    "\n",
    "# Remove extra spaces\n",
    "text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "# Remove single dots followed by space\n",
    "text = re.sub(r'\\.\\s', '. ', text)\n",
    "\n",
    "# Replace punctuations with a space, except for periods to retain sentence separation\n",
    "text = re.sub(r'[^\\w\\s\\.]', '', text)\n",
    "\n",
    "# Remove URLs\n",
    "text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "# Remove redundant spaces\n",
    "text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Convert to lowercase\n",
    "text = text.lower()\n",
    "# Remove numbers\n",
    "text = re.sub(r'\\d', '', text)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef46e1c7-bc8b-448d-a05d-3e9a3fbaca45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count (excluding stop words and punctuation): 256\n"
     ]
    }
   ],
   "source": [
    "# Define stop words and punctuation\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def clean_and_count_words(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation and stop words\n",
    "    cleaned_words = [word.lower() for word in words if word.lower() not in stop_words and word not in punctuation]\n",
    "    \n",
    "    # Count the number of cleaned words\n",
    "    word_count = len(cleaned_words)\n",
    "    return word_count\n",
    "\n",
    "# Calculate word count\n",
    "word_count = clean_and_count_words(text)\n",
    "print(f\"Word Count (excluding stop words and punctuation): {word_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "468d530c-2d68-4af4-8a4e-13202555072f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syllable Counts per Word:\n",
      "Syllable Counts per Word:\n",
      "client: 2 (Most Common)\n",
      "background: 2 (Most Common)\n",
      "leading: 2 (Most Common)\n",
      "healthtech: 0\n",
      "firm: 1\n",
      "usa: 3\n",
      "industry: 3\n",
      "type: 1\n",
      "healthcare: 2 (Most Common)\n",
      "products: 2 (Most Common)\n",
      "services: 3\n",
      "medical: 3\n",
      "solutions: 3\n",
      "organization: 5\n",
      "size: 1\n",
      "problem: 2 (Most Common)\n",
      "efficiently: 4\n",
      "create: 2 (Most Common)\n",
      "orthopedic: 4\n",
      "case: 1\n",
      "reports: 2 (Most Common)\n",
      "extracting: 3\n",
      "data: 2 (Most Common)\n",
      "online: 2 (Most Common)\n",
      "sources: 2 (Most Common)\n",
      "including: 3\n",
      "articles: 3\n",
      "videos: 3\n",
      "user: 2 (Most Common)\n",
      "comments: 2 (Most Common)\n",
      ".: 0\n",
      "involves: 2 (Most Common)\n",
      "summarizing: 4\n",
      "citing: 2 (Most Common)\n",
      "relevant: 3\n",
      "pubmed.gov: 0\n",
      "past: 1\n",
      "years: 1\n",
      "related: 3\n",
      "requires: 3\n",
      "automating: 4\n",
      "extraction: 3\n",
      "summarization: 0\n",
      "websites: 0\n",
      "making: 2 (Most Common)\n",
      "timeconsuming: 0\n",
      "task: 1\n",
      "manually: 4\n",
      "solution: 3\n",
      "develops: 3\n",
      "python: 2 (Most Common)\n",
      "tool: 1\n",
      "accepts: 2 (Most Common)\n",
      "website: 2 (Most Common)\n",
      "url: 0\n",
      "input: 2 (Most Common)\n",
      "generates: 3\n",
      "report: 2 (Most Common)\n",
      "integrates: 3\n",
      "web: 1\n",
      "scraping: 2 (Most Common)\n",
      "extract: 2 (Most Common)\n",
      "utilizes: 4\n",
      "ai: 1\n",
      "chatgpt: 0\n",
      "creating: 3\n",
      "summaries: 3\n",
      "responses: 3\n",
      "leverages: 0\n",
      "pubmed: 0\n",
      "recent: 2 (Most Common)\n",
      "application: 4\n",
      "userfriendly: 0\n",
      "access: 2 (Most Common)\n",
      "capabilities: 5\n",
      "architecture: 4\n",
      "techniques: 2 (Most Common)\n",
      "gather: 2 (Most Common)\n",
      "trusted: 2 (Most Common)\n",
      "combines: 2 (Most Common)\n",
      "generating: 4\n",
      "responding: 3\n",
      "queries: 2 (Most Common)\n",
      "retrieving: 3\n",
      "deploys: 2 (Most Common)\n",
      "interaction: 4\n",
      "deliverables: 0\n",
      "project: 2 (Most Common)\n",
      "github: 0\n",
      "source: 1\n",
      "code: 1\n",
      "tech: 1\n",
      "stack: 1\n",
      "tools: 1\n",
      "beautifulsoup: 0\n",
      "requests: 2 (Most Common)\n",
      "languagetechniques: 0\n",
      "models: 2 (Most Common)\n",
      "skills: 1\n",
      "webscraping: 0\n",
      "prompting: 2 (Most Common)\n",
      "databases: 4\n",
      "cloud: 1\n",
      "servers: 2 (Most Common)\n",
      "technical: 3\n",
      "challenges: 3\n",
      "faced: 1\n",
      "execution: 4\n",
      "accurate: 3\n",
      "reliable: 4\n",
      "diverse: 2 (Most Common)\n",
      "integration: 4\n",
      "components: 3\n",
      "text: 1\n",
      "generation: 4\n",
      "efficient: 3\n",
      "querying: 0\n",
      "retrieval: 3\n",
      "handling: 2 (Most Common)\n",
      "formats: 2 (Most Common)\n",
      "structures: 2 (Most Common)\n",
      "developing: 4\n",
      "interface: 3\n",
      "solved: 1\n",
      "extensive: 3\n",
      "research: 2 (Most Common)\n",
      "testing: 2 (Most Common)\n",
      "libraries: 3\n",
      "utilization: 5\n",
      "api: 0\n",
      "article: 3\n",
      "custom: 2 (Most Common)\n",
      "parsers: 2 (Most Common)\n",
      "collaboration: 5\n",
      "experts: 2 (Most Common)\n",
      "design: 2 (Most Common)\n",
      "feedback: 2 (Most Common)\n",
      "summarize: 3\n",
      "summarized: 3\n",
      "blackcoffer: 0\n",
      "team: 1\n",
      "global: 2 (Most Common)\n",
      "consulting: 3\n",
      "contact: 2 (Most Common)\n",
      "details: 2 (Most Common)\n",
      "designed: 2 (Most Common)\n",
      "developed: 3\n",
      "pvt: 0\n",
      "ltd.: 0\n",
      "www.blackcoffer.com: 0\n",
      "address: 2 (Most Common)\n",
      "eextension: 0\n",
      "shaym: 0\n",
      "vihar: 0\n",
      "phase: 1\n",
      "delhi: 2 (Most Common)\n",
      "email: 2 (Most Common)\n",
      "ajayblackcoffer.com: 0\n",
      "skype: 0\n",
      "asbidyarthy: 0\n",
      "whatsapp: 0\n",
      "telegram: 3\n",
      "\n",
      "Word(s) with the most common syllable count (2 syllables):\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def syllable_count_per_word(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    syllable_counts = {word: count_syllables(word) for word in words}\n",
    "    return syllable_counts\n",
    "\n",
    "\n",
    "\n",
    "# Calculate syllable count per word\n",
    "syllable_counts = syllable_count_per_word(text)\n",
    "print(\"Syllable Counts per Word:\")\n",
    "# Count occurrences of each syllable count\n",
    "syllable_count_frequency = Counter(syllable_counts.values())\n",
    "\n",
    "# Find the most common syllable count\n",
    "most_common_syllable_count, _ = syllable_count_frequency.most_common(1)[0]\n",
    "\n",
    "# Find all words with the most common syllable count\n",
    "most_common_syllable_words = [word for word, count in syllable_counts.items() if count == most_common_syllable_count]\n",
    "\n",
    "# Print syllable counts and highlight the most common syllable count\n",
    "print(\"Syllable Counts per Word:\")\n",
    "for word, count in syllable_counts.items():\n",
    "    if count == most_common_syllable_count:\n",
    "        print(f\"{word}: {count} (Most Common)\")\n",
    "    else:\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "# Print the word(s) with the most common syllable count separately\n",
    "print(f\"\\nWord(s) with the most common syllable count ({most_common_syllable_count} syllables):\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75fd2f3f-1352-47ca-8f45-694814ca752d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal Pronoun Counts:\n",
      "I: 0\n",
      "we: 0\n",
      "my: 0\n",
      "ours: 0\n",
      "us: 0\n",
      "\n",
      "Total Count of Personal Pronouns: 0\n"
     ]
    }
   ],
   "source": [
    "def count_personal_pronouns(text):\n",
    "    # Define personal pronouns and a pattern to avoid 'US' as a country\n",
    "    pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "    \n",
    "    # Define a regex pattern to match personal pronouns but not 'US' as a country\n",
    "    # Use negative lookbehind and lookahead to ensure 'US' is not mistakenly counted\n",
    "    pronoun_pattern = r'\\b(?:I|we|my|ours)\\b|\\b(?:us)\\b'\n",
    "    \n",
    "    # Compile the regex pattern\n",
    "    pattern = re.compile(pronoun_pattern, re.IGNORECASE)\n",
    "    \n",
    "    # Find all matches in the text\n",
    "    matches = pattern.findall(text)\n",
    "    \n",
    "    # Initialize a dictionary to store the counts of each pronoun\n",
    "    pronoun_counts = {pronoun: 0 for pronoun in pronouns}\n",
    "    \n",
    "    # Count occurrences of each personal pronoun\n",
    "    for match in matches:\n",
    "        if match.lower() in pronoun_counts:\n",
    "            pronoun_counts[match.lower()] += 1\n",
    "    \n",
    "    # Calculate the total count of all personal pronouns\n",
    "    total_count = sum(pronoun_counts.values())\n",
    "    \n",
    "    return pronoun_counts, total_count\n",
    "\n",
    "\n",
    "# Calculate personal pronoun counts\n",
    "pronoun_counts, total_count = count_personal_pronouns(text)\n",
    "\n",
    "# Print individual pronoun counts and total count\n",
    "print(\"Personal Pronoun Counts:\")\n",
    "for pronoun, count in pronoun_counts.items():\n",
    "    print(f\"{pronoun}: {count}\")\n",
    "\n",
    "print(f\"\\nTotal Count of Personal Pronouns: {total_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "283fc618-8b9a-496a-8691-3ac658203e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Word Length: 7.33\n"
     ]
    }
   ],
   "source": [
    "def calculate_average_word_length(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove any punctuation and non-alphabetic characters from each word\n",
    "    cleaned_words = [re.sub(r'\\W+', '', word) for word in words if word.isalpha()]\n",
    "    \n",
    "    # Calculate the total number of characters in each word\n",
    "    total_characters = sum(len(word) for word in cleaned_words)\n",
    "    \n",
    "    # Calculate the total number of words\n",
    "    total_words = len(cleaned_words)\n",
    "    \n",
    "    # Calculate the average word length\n",
    "    average_word_length = total_characters / total_words if total_words > 0 else 0\n",
    "    average_word_length=round(average_word_length, 2)\n",
    "    \n",
    "    return average_word_length\n",
    "\n",
    "\n",
    "\n",
    "# Calculate average word length\n",
    "average_word_length = calculate_average_word_length(text)\n",
    "print(f\"Average Word Length: {average_word_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fad0b31-8ebb-45e8-9c2c-eb8ca5ab54c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['POSITIVE SCORE', 'NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c69973e-f24c-4c76-a614-bee7062716d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_21444\\568878410.py:1: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = df._append(pd.DataFrame({'POSITIVE SCORE': positive_score,\n"
     ]
    }
   ],
   "source": [
    "df = df._append(pd.DataFrame({'POSITIVE SCORE': positive_score,\n",
    "                                                        'NEGATIVE SCORE': negative_score,\n",
    "                                                        'POLARITY SCORE':polarity_score,\n",
    "                                'SUBJECTIVITY SCORE':subjectivity_score,\n",
    "                               'AVG SENTENCE LENGTH':average_sentence_length,\n",
    "                               'PERCENTAGE OF COMPLEX WORDS':percentage_of_complex_words,\n",
    "                               'FOG INDEX':fog_index,\n",
    "                              'AVG NUMBER OF WORDS PER SENTENCE':average_sentence_length,\n",
    "                               'COMPLEX WORD COUNT':complex_word_count,\n",
    "                               'WORD COUNT':word_count,\n",
    "                               'SYLLABLE PER WORD':most_common_syllable_count,\n",
    "                               'PERSONAL PRONOUNS':total_count,\n",
    "                               'AVG WORD LENGTH':average_word_length},index=[0]), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de9fcb70-1d62-4e99-ae16-02802250d08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.030717</td>\n",
       "      <td>11.72</td>\n",
       "      <td>27.65</td>\n",
       "      <td>15.75</td>\n",
       "      <td>11.72</td>\n",
       "      <td>81</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  POSITIVE SCORE NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0              3              6       -0.333333            0.030717   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0                11.72                        27.65      15.75   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE COMPLEX WORD COUNT WORD COUNT  \\\n",
       "0                             11.72                 81        256   \n",
       "\n",
       "  SYLLABLE PER WORD PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0                 2                 0             7.33  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8894d162-f7d7-4ef2-9275-6bc6fc434c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('Output Data Structure.xlsx', sheet_name='Sheet1', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054b6c9-d8e2-4cc9-a061-47c16066c10b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd98467-3310-4120-9b9f-639ad06971a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b7e32-2643-4be3-b132-9f5103c17452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a43f5-2795-452b-8844-97a9aa80465a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b0505f-dc6a-4c83-8b5b-035e2cfa14b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d7d1c-78c4-4091-aa5e-d56bfeb2bcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ad145-974c-4daf-bce6-6143548ddc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
